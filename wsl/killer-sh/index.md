cd /home/wsl/dev/k8s-ckad/wsl/script
make reset-heavy
cd /home/wsl/dev/k8s-ckad/wsl/killer-sh

alias k=kubectl
export do="--dry-run=client -o yaml"
alias kn='kubectl config set-context --current --namespace '

====================================
Q1

The DevOps team would like to get the list of all Namespaces in the cluster.  
Get the list and save it to ~/dev/k8s-ckad/wsl/killer-sh/namespaces
====================================


s-ckad/wsl/killer-sh$ k get ns -A > ~/dev/k8s-ckad/wsl/killer-sh/namespaces
wsl@DESKTOP-M40H3KM:~/dev/k8s-ckad/wsl/killer-sh$ cat ~/dev/k8s-ckad/wsl/killer-sh/namespaces
NAME              STATUS   AGE
default           Active   3m3s
kube-node-lease   Active   3m3s
kube-public       Active   3m3s
kube-system       Active   3m3s


====================================
Q2

Question 2:
Solve this question on instance: ssh ckad5601

Create a single Pod of image httpd:2.4.41-alpine in Namespace default.  
The Pod should be named pod1 and the container should be named pod1-container.

Your manager would like to run a command manually on occasion to output the status of that exact Pod.  
Please write whilea command that does this into /home/wsl/dev/k8s-ckad/wsl/killer-sh/pod1-status-command.sh on ckad5601. The command should use kubectl.

====================================

40H3KM:~/dev/k8s-ckad/wsl/killer-sh$ while true end; kubectl get pod pod1; end; > pod1-status-command.sh
> ^C
wsl@DESKTOP-M40H3KM:~/dev/k8s-ckad/wsl/killer-sh$ cat pod1.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod1
  name: pod1
spec:
  containers:
  - image: httpd:2.4.41-alpine
    name: pod1-container 
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
wsl@DESKTOP-M40H3KM:~/dev/k8s-ckad/wsl/killer-sh$ 



====================================
Q3

Question 3:
Solve this question on instance: ssh ckad7326

Team Neptune needs a Job template located at job.yaml.  
This Job should run image busybox:1.31.0 and execute sleep 2 && echo done.  
It should be in namespace neptune, run a total of 3 times and should execute 2 runs in parallel.

Start the Job and check its history. Each pod created by the Job should have the label id: awesome-job.  
The job should be named neb-new-job and the container neb-new-job-container.

====================================

k create job neb-new-job --image=busybox:1.31.0 -n neptune     -- "sleep 2 && echo done"    --dry-run=client -o yaml > job.yaml
completion:3
parallel:2


====================================
Q4

Question 4:
Solve this question on instance: ssh ckad7326

Team Mercury asked you to perform some operations using Helm, all in Namespace mercury:

1. Delete release internal-issue-report-apiv1  
2. Upgrade release internal-issue-report-apiv2 to any newer version of chart bitnami/nginx available  
3. Install a new release internal-issue-report-apache of chart bitnami/apache.  
   The Deployment should have two replicasâ€”set these via Helm-values during install  
4. There seems to be a broken release, stuck in pending-install state. Find it and delete it

====================================
helm uninstall internal-issue-report-apiv1  
helm upgrade internal-issue-report-apiv2 bitnami/nginx
helm install internal-issue-report-apache bitnami/apache

====================================
Q5

Question 5:
Solve this question on instance: ssh ckad7326

Team Neptune has its own ServiceAccount named neptune-sa-v2 in Namespace neptune.  
A coworker needs the token from the Secret that belongs to that ServiceAccount.  
Write the base64 decoded token to file /opt/course/5/token on ckad7326.

01-neptune-sa.yaml

apiVersion: v1
kind: Namespace
metadata:
  name: neptune
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: neptune-sa-v2
  namespace: neptune
  # å¿…é ˆã§ã¯ãªã„ãŒæ˜ç¤ºã—ã¦ãŠãã¨åˆ†ã‹ã‚Šã‚„ã™ã„
secrets:
  - name: neptune-secret-1    # â†“â‘¡ã§ä½œæˆã™ã‚‹ Secret åã‚’å‚ç…§



02-neptune-secret.yaml

apiVersion: v1
kind: Secret
metadata:
  name: neptune-secret-1
  namespace: neptune
  annotations:
    kubernetes.io/service-account.name: neptune-sa-v2
type: kubernetes.io/service-account-token


====================================



====================================
Q6

Question 6:
Solve this question on instance: ssh ckad5601

Create a single Pod named pod6 in Namespace default of image busybox:1.31.0.  
The Pod should have a readiness-probe executing `cat /tmp/ready`. It should initially wait 5 seconds and then probe every 10 seconds. This will set the container ready only if the file `/tmp/ready` exists.

The Pod should run the command `touch /tmp/ready && sleep 1d`, which will create the necessary file to become ready and then idle. Create the Pod and confirm it starts.

====================================

k run pod pod6 --image=busybox:1.31.0 --dry-run=client -oyaml --command -- sh -c    "touch /tmp/ready && sleep 1d"     > pod6.yaml


====================================
Q7

Question 7:
Solve this question on instance: ssh ckad7326

The board of Team Neptune decided to take over control of one e-commerce webserver from Team Saturn. The administrator who once set up this webserver is no longer part of the organization. All information you could get was that the e-commerce system is called my-happy-shop.

Search for the correct Pod in Namespace saturn and move it to Namespace neptune. It doesnâ€™t matter if you shut it down and spin it up again; it probably hasnâ€™t any customers anyways.

07-namespaces.yaml

apiVersion: v1
kind: Namespace
metadata:
  name: saturn
---
apiVersion: v1
kind: Namespace
metadata:
  name: neptune


07-webservers-saturn.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: webserver-sat-001
  namespace: saturn
  labels:
    id: webserver-sat-001
spec:
  containers:
    - name: nginx
      image: nginx:1.16.1-alpine
      ports: [{ containerPort: 80 }]
  restartPolicy: Always
---
apiVersion: v1
kind: Pod
metadata:
  name: webserver-sat-002
  namespace: saturn
  labels:
    id: webserver-sat-002
spec:
  containers:
    - name: nginx
      image: nginx:1.16.1-alpine
      ports: [{ containerPort: 80 }]
  restartPolicy: Always
---
apiVersion: v1
kind: Pod
metadata:
  name: webserver-sat-003                # â† ã“ã‚ŒãŒ â€œmy-happy-shopâ€
  namespace: saturn
  labels:
    id: webserver-sat-003
  annotations:
    description: >-
      this is the server for the e-Commerce System my-happy-shop
spec:
  containers:
    - name: nginx
      image: nginx:1.16.1-alpine
      ports: [{ containerPort: 80 }]
  restartPolicy: Always
---
apiVersion: v1
kind: Pod
metadata:
  name: webserver-sat-004
  namespace: saturn
  labels:
    id: webserver-sat-004
spec:
  containers:
    - name: nginx
      image: nginx:1.16.1-alpine
      ports: [{ containerPort: 80 }]
  restartPolicy: Always
---
apiVersion: v1
kind: Pod
metadata:
  name: webserver-sat-005
  namespace: saturn
  labels:
    id: webserver-sat-005
spec:
  containers:
    - name: nginx
      image: nginx:1.16.1-alpine
      ports: [{ containerPort: 80 }]
  restartPolicy: Always
---
apiVersion: v1
kind: Pod
metadata:
  name: webserver-sat-006
  namespace: saturn
  labels:
    id: webserver-sat-006
spec:
  containers:
    - name: nginx
      image: nginx:1.16.1-alpine
      ports: [{ containerPort: 80 }]
  restartPolicy: Always

====================================




====================================
Q8

Question 8:
Solve this question on instance: ssh ckad7326

There is an existing Deployment named api-new-c32 in Namespace neptune. A developer made an update to the Deployment but the updated version never came online. Check the Deploymentâ€™s revision history, find a revision that works, then rollback to it. Could you tell Team Neptune what the error was so it doesnâ€™t happen again?

08-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: neptune


08-deploy-v1.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-new-c32
  namespace: neptune
spec:
  replicas: 3
  selector:
    matchLabels: { app: api-new-c32 }
  template:
    metadata:
      labels: { app: api-new-c32 }
    spec:
      containers:
        - name: backend
          image: nginx:1.23-alpine        # âœ… pull å¯èƒ½
          ports: [{ containerPort: 80 }]

08-deploy-v2-bad.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-new-c32
  namespace: neptune
spec:
  replicas: 3
  selector:
    matchLabels: { app: api-new-c32 }
  template:
    metadata:
      labels: { app: api-new-c32 }
      annotations:
        commit: bad-v2                  # â† â˜… ã‚ã–ã¨ 1 è¡Œè¿½åŠ ã—ã¦å·®åˆ†ã‚’ç¢ºå®ŸåŒ–
    spec:
      containers:
        - name: backend
          image: nginx:9.99-does-not-exist   # âŒ ImagePullBackOff
          ports: [{ containerPort: 80 }]


====================================




====================================
Q9

Question 9:
Solve this question on instance: ssh ckad9043

In Namespace pluto there is a single Pod named holy-api. It has been working okay for a while now but Team Pluto needs it to be more reliable.

Convert the Pod into a Deployment named holy-api with 3 replicas and delete the original Pod once done. The raw Pod template file is available at /opt/course/9/holy-api-pod.yaml.

In addition, the new Deployment should set `allowPrivilegeEscalation: false` and `privileged: false` in the containerâ€™s securityContext.  
Please create the Deployment and save its YAML under /opt/course/9/holy-api-deployment.yaml.

09-pluto-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: pluto

09-holy-api-pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: holy-api
  namespace: pluto
  labels:
    app: holy-api
spec:
  containers:
    - name: api
      image: nginx:1.23-alpine   # ä¾‹:è»½é‡ã§å‹•ä½œç¢ºèªã—ã‚„ã™ã„
      ports:
        - containerPort: 80



====================================




====================================
Q10

Question 10:
Solve this question on instance: ssh ckad9043

Team Pluto needs a new cluster internal Service.  
Create a ClusterIP Service named project-plt-6cc-svc in Namespace pluto.  
This Service should expose a single Pod named project-plt-6cc-api of image nginx:1.17.3-alpineâ€”create that Pod as well.  
The Pod should be identified by label project: plt-6cc-api.  
The Service should use tcp port redirection of 3333:80.

Finally, useâ€”for exampleâ€”curl from a temporary nginx:alpine Pod to get the response from the Service.  
Write the response into /opt/course/10/service_test.html on ckad9043.  
Also check if the logs of Pod project-plt-6cc-api show the request and write those into /opt/course/10/service_test.log on ckad9043.

====================================

ã»ã¼å½¢ã«ãªã£ã¦ã„ã¾ã™ãŒã€**è©¦é¨“è¦ä»¶ã©ãŠã‚Šã«å‹•ãã‚ˆã†ã«ã™ã‚‹ã«ã¯ 3 ç‚¹ã ã‘ä¿®æ­£** ãŒå¿…è¦ã§ã™ã€‚

| #     | ä¿®æ­£ãƒã‚¤ãƒ³ãƒˆ             | ç¾çŠ¶                              | æ­£ã—ãã¯                                                                            |
| ----- | ------------------ | ------------------------------- | ------------------------------------------------------------------------------- |
| **1** | **Pod ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚¿ã‚°**     | `nginx:1.17.3`                  | `nginx:1.17.3-alpine`<br>ï¼ˆå•é¡Œæ–‡ã©ãŠã‚Š â€œ-alpineâ€ ã‚’ä»˜ã‘ã‚‹ï¼‰                                |
| **2** | **Service ã®ãƒãƒ¼ãƒˆå¯¾å¿œ** | `port: 80` â†’ `targetPort: 3333` | **é€†** ã«ã™ã‚‹ï¼š<br>`port: 3333`ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒå©ãå´ï¼‰<br>`targetPort: 80`ï¼ˆPod å†… Nginx ãŒå¾…ã¡å—ã‘ã‚‹å´ï¼‰ |
| **3** | **Service ä½œæˆã‚³ãƒãƒ³ãƒ‰** | `-l plt-6cc-api` ã§ã‚¨ãƒ©ãƒ¼           | `--labels project=plt-6cc-api`<br>ï¼ˆ`-l` ã¯ selector ç”¨ãƒ»labels ã¯ key=valueï¼‰        |

---

## âœ” ä¿®æ­£å¾Œã® YAML ã‚µãƒ³ãƒ—ãƒ«

### Pod - `project-plt-6cc-api.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: project-plt-6cc-api
  namespace: pluto
  labels:
    project: plt-6cc-api
spec:
  containers:
    - name: project-plt-6cc-api
      image: nginx:1.17.3-alpine          # â† ä¿®æ­£â‘ 
      ports:
        - containerPort: 80
```

### Service - `project-plt-6cc-svc.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: project-plt-6cc-svc
  namespace: pluto
spec:
  selector:
    project: plt-6cc-api
  ports:
    - port: 3333         # â† ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå—ä»˜
      targetPort: 80     # â† Pod å†…
      protocol: TCP
```

---

## ğŸ”„ ä½œã‚Šç›´ã—ã‚³ãƒãƒ³ãƒ‰ä¾‹

```bash
# Namespace ãŒç„¡ã‘ã‚Œã°
kubectl create ns pluto --dry-run=client -o yaml | kubectl apply -f -

# Pod ã‚’é©ç”¨
kubectl apply -f project-plt-6cc-api.yaml

# Service ã‚’é©ç”¨
kubectl apply -f project-plt-6cc-svc.yaml
```

> `kubectl expose` ã§ã‚µã‚¯ãƒƒã¨ä½œã‚‹å ´åˆã¯
>
> ```bash
> kubectl expose pod project-plt-6cc-api \
>   --name=project-plt-6cc-svc \
>   --port=3333 --target-port=80 \        # â† ãƒãƒ¼ãƒˆã‚’é€†ã«æŒ‡å®š
>   --namespace=pluto \
>   --dry-run=client -o yaml > project-plt-6cc-svc.yaml
> ```

---

## ğŸ§ª å‹•ä½œãƒ†ã‚¹ãƒˆ & ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›

```bash
# Pod Ready ã‚’å¾…ã¤
kubectl wait --for=condition=ready pod/project-plt-6cc-api -n pluto --timeout=60s

# 1ï¸âƒ£ Service çµŒç”±ã§ HTML ã‚’å–å¾—
kubectl run tmp-curl -n pluto --rm -i --restart=Never \
  --image=nginx:alpine --command -- \
  sh -c "apk add --no-cache curl >/dev/null && \
         curl -s http://project-plt-6cc-svc:3333/" \
  > /opt/course/10/service_test.html

# 2ï¸âƒ£ Pod ã®ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ä¿å­˜
kubectl logs project-plt-6cc-api -n pluto \
  > /opt/course/10/service_test.log
```

---

### ã“ã‚Œã§ OK ã«ãªã‚‹ç†ç”±

* **Pod ãƒ©ãƒ™ãƒ«** ã¨ **Service selector** ãŒä¸€è‡´
* Service **port 3333 â†’ targetPort 80** ã§è¦ä»¶ã©ãŠã‚Šã® TCP ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
* ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚¿ã‚°ã‚‚å•é¡Œæ–‡æŒ‡å®š `1.17.3-alpine` ã«ä¿®æ­£æ¸ˆã¿

ä»¥ä¸Š 3 ç‚¹ç›´ã›ã°ã€`curl http://project-plt-6cc-svc:3333` ã§ HTML ãŒå–ã‚Œã€ãƒ­ã‚°ã«ã‚‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå‡ºã¾ã™ï¼



====================================
Q11

Question 11:
Solve this question on instance: ssh ckad9043

During the last monthly meeting you mentioned your strong expertise in container technology.  
Now the Build&Release team of department Sun is in need of your insight knowledge.  
There are files to build a container image located at /opt/course/11/image.  
The container will run a Golang application which outputs information to stdout.  
You're asked to perform the following tasks:

NOTE: Make sure to run all commands as user candidate; for docker use sudo docker

1. Change the Dockerfile. The value of the environment variable SUN_CIPHER_ID should be set to the hardcoded value 5b9c1065-e39d-4a43-a04a-e59bcea3e03f  
2. Build the image using Docker, named registry.killer.sh:5000/sun-cipher, tagged as latest and v1-docker; push these to the registry  
3. Build the image using Podman, named registry.killer.sh:5000/sun-cipher, tagged as v1-podman; push it to the registry  
4. Run a container using Podman, which keeps running in the background, named sun-cipher using image registry.killer.sh:5000/sun-cipher:v1-podman.  
   Run the container from candidate@ckad9043 and not root@ckad9043  
5. Write the logs your container sun-cipher produced into /opt/course/11/logs.  
   Then write a list of all running Podman containers into /opt/course/11/containers on ckad9043.


====================================
dockerã¯å¾Œå›ã—




====================================
Q12

Question 12:
Solve this question on instance: ssh ckad5601

Create a new PersistentVolume named earth-project-earthflower-pv.  
It should have a capacity of 2Gi, accessMode ReadWriteOnce, hostPath /Volumes/Data and no storageClassName defined.

Next, create a new PersistentVolumeClaim in Namespace earth named earth-project-earthflower-pvc.  
It should request 2Gi storage, accessMode ReadWriteOnce and should not define a storageClassName.  
The PVC should be bound to the PV correctly.

Finally, create a new Deployment project-earthflower in Namespace earth which mounts that volume at /tmp/project-data.  
The Pods of that Deployment should be of image httpd:2.4.41-alpine.

====================================





====================================
Q13

Question 13:
Solve this question on instance: ssh ckad9043

Team Moonpie, which has the Namespace moon, needs more storage.  
Create a new PersistentVolumeClaim named moon-pvc-126 in that namespace.  
This claim should use a new StorageClass moon-retain with the provisioner set to moon-retainer and the reclaimPolicy set to Retain.  
The claim should request storage of 3Gi, an accessMode of ReadWriteOnce and should use the new StorageClass.

The provisioner moon-retainer will be created by another team, so it's expected that the PVC will not bind yet.  
Confirm this by writing the log message from the PVC into file /opt/course/13/pvc-126-reason.

====================================


apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: moon-retain
provisioner: moon-retainer          # â˜…å¿…é ˆ
reclaimPolicy: Retain               # â˜…è¦ä»¶

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: moon-pvc-126
  namespace: moon
spec:
  accessModes:
    - ReadWriteOnce                 # â˜…è¦ä»¶
  resources:
    requests:
      storage: 3Gi                  # â˜…è¦ä»¶
  storageClassName: moon-retain     # â˜…è¦ä»¶




====================================
Q14

Question 14:
Solve this question on instance: ssh ckad9043

You need to make changes on an existing Pod in Namespace moon called secret-handler.  
Create a new Secret secret1 which contains user=test and pass=pwd.  
The Secretâ€™s content should be available in Pod secret-handler as environment variables SECRET1_USER and SECRET1_PASS.  
The YAML for Pod secret-handler is available at /opt/course/14/secret-handler.yaml.

There is existing YAML for another Secret at /opt/course/14/secret2.yaml; create this Secret and mount it inside the same Pod at /tmp/secret2.  
Your changes should be saved under /opt/course/14/secret-handler-new.yaml on ckad9043.  
Both Secrets should only be available in Namespace moon.

secret1.yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret1
  namespace: moon
type: Opaque            # â† æ–‡å­—åˆ—ãã®ã¾ã¾æ‰±ãˆã‚‹ã‚ˆã† stringData ã‚’ä½¿ç”¨
stringData:
  user: test
  pass: pwd

secret-handler-new.yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-handler
  namespace: moon
  labels:                # (å…ƒã®ãƒ©ãƒ™ãƒ«ãŒã‚ã‚Œã°ãã®ã¾ã¾)
    app: secret-handler
spec:
  containers:
  - name: secret-handler
    image: # ... å…ƒã® image ...
    # ... (command / args / ports ãªã©æ—¢å­˜å®šç¾©) ...
    env:
      - name: SECRET1_USER
        valueFrom:
          secretKeyRef:
            name: secret1
            key: user
      - name: SECRET1_PASS
        valueFrom:
          secretKeyRef:
            name: secret1
            key: pass
    volumeMounts:
      - name: secret2-vol
        mountPath: /tmp/secret2
        readOnly: true
      # ... æ—¢å­˜ volumeMount ãŒã‚ã‚Œã°ã“ã“ã«æ®‹ã™ ...
  volumes:
    - name: secret2-vol
      secret:
        secretName: secret2
        defaultMode: 0440
    # ... æ—¢å­˜ volumes ãŒã‚ã‚Œã°ã“ã“ã«æ®‹ã™ ...

====================================






====================================
Q15

Question 15:
Solve this question on instance: ssh ckad9043

Team Moonpie has an nginx server Deployment called web-moon in Namespace moon.  
Someone started configuring it but it was never completed.  
To complete, please create a ConfigMap called configmap-web-moon-html containing the content of file /opt/course/15/web-moon.html under the data key-name index.html.

The Deployment web-moon is already configured to work with this ConfigMap and serve its content.  
Test the nginx configuration, for example using curl from a temporary nginx:alpine Pod.

apiVersion: v1
kind: ConfigMap
metadata:
  name: configmap-web-moon-html
  namespace: moon
data:
  index.html: |
    <!-- ã“ã“ã« /opt/course/15/web-moon.html ã®å†…å®¹ã‚’è²¼ã‚Šä»˜ã‘ã‚‹ -->

====================================




====================================
Q16

Question 16:
Solve this question on instance: ssh ckad7326

The Tech Lead of Mercury2D decided itâ€™s time for more logging, to finally fight all these missing data incidents.  
There is an existing container named cleaner-con in Deployment cleaner in Namespace mercury.  
This container mounts a volume and writes logs into a file called cleaner.log.

The YAML for the existing Deployment is available at /opt/course/16/cleaner.yaml.  
Persist your changes at /opt/course/16/cleaner-new.yaml on ckad7326 but also make sure the Deployment is running.

Create a sidecar container named logger-con, image busybox:1.31.0, which mounts the same volume and writes the content of cleaner.log to stdout (you can use tail -f for this).  
This way it can be picked up by kubectl logs.  
Check if the logs of the new container reveal something about the missing data incidents.

cleaner-new.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cleaner
  namespace: mercury
  labels:
    app: cleaner
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cleaner
  template:
    metadata:
      labels:
        app: cleaner
    spec:
      containers:
      # --- ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚³ãƒ³ãƒ†ãƒŠ ----------------------------------
      - name: cleaner-con
        image: busybox:1.31.0        # æ—¢å­˜ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒä¸æ˜ãªå ´åˆã®ã‚µãƒ³ãƒ—ãƒ«
        command: ["sh", "-c", "while true; do echo \"$(date) - cleaning job ran\" >> /var/log/cleaner/cleaner.log; sleep 10; done"]
        volumeMounts:
        - name: logs-vol
          mountPath: /var/log/cleaner
      # --- è¿½åŠ ã™ã‚‹ã‚µã‚¤ãƒ‰ã‚«ãƒ¼ ----------------------------------
      - name: logger-con
        image: busybox:1.31.0
        command: ["sh", "-c", "tail -F /var/log/cleaner/cleaner.log"]
        volumeMounts:
        - name: logs-vol
          mountPath: /var/log/cleaner
      # --------------------------------------------------------
      volumes:
      - name: logs-vol
        emptyDir: {}                 # ãƒ­ã‚°ä¿å­˜ç”¨ã«ä¸¡ã‚³ãƒ³ãƒ†ãƒŠãŒå…±æœ‰




====================================




====================================
Q17

Question 17:
Solve this question on instance: ssh ckad5601

Last lunch you told your coworker from department Mars Inc how amazing InitContainers are.  
Now he would like to see one in action.  
There is a Deployment YAML at /opt/course/17/test-init-container.yaml.  
This Deployment spins up a single Pod of image nginx:1.17.3-alpine and serves files from a mounted volume, which is empty right now.

Create an InitContainer named init-con which also mounts that volume and creates a file index.html with content "check this out!" in the root of the mounted volume.  
For this test we ignore that it doesn't contain valid HTML.

The InitContainer should be using image busybox:1.31.0.  
Test your implementation, for example using curl from a temporary nginx:alpine Pod.

test-init-container.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-init
  namespace: mars          # å¥½ããª Namespace ãŒã‚ã‚Œã°å¤‰æ›´ã—ã¦ãã ã•ã„
  labels:
    app: test-init
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-init
  template:
    metadata:
      labels:
        app: test-init
    spec:
      volumes:
        - name: content-vol      # â˜… nginx ã¨ InitContainer ã§å…±æœ‰ã™ã‚‹ãƒœãƒªãƒ¥ãƒ¼ãƒ 
          emptyDir: {}
      # ---- ã“ã‚Œã‹ã‚‰è¿½åŠ ã™ã‚‹ InitContainer ç”¨ã®ã‚¹ãƒšãƒ¼ã‚¹ -----------------
      # initContainers:
      #   - name: init-con
      #     image: busybox:1.31.0
      #     command: ["sh", "-c", "echo 'check this out!' > /usr/share/nginx/html/index.html"]
      #     volumeMounts:
      #       - name: content-vol
      #         mountPath: /usr/share/nginx/html
      # ------------------------------------------------------------------
      containers:
        - name: nginx
          image: nginx:1.17.3-alpine
          ports:
            - containerPort: 80
          volumeMounts:
            - name: content-vol
              mountPath: /usr/share/nginx/html


====================================




====================================
Q18

Question 18:
Solve this question on instance: ssh ckad5601

There seems to be an issue in Namespace mars where the ClusterIP service manager-api-svc should make the Pods of Deployment manager-api-deployment available inside the cluster.

You can test this with curl manager-api-svc.mars:4444 from a temporary nginx:alpine Pod.  
Check for the misconfiguration and apply a fix.

mars-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: mars

manager-api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: manager-api-deployment
  namespace: mars
  labels:
    app: manager-api
spec:
  replicas: 2
  selector:
    matchLabels:
      app: manager-api
  template:
    metadata:
      labels:
        app: manager-api
    spec:
      containers:
        - name: manager-api
          image: nginx:1.17.3-alpine
          ports:
            - containerPort: 80

manager-api-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: manager-api-svc
  namespace: mars
spec:
  type: ClusterIP
  selector:
    app: manager-api
  ports:
    - name: http
      port: 4444          # â† ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãƒãƒ¼ãƒˆ
      targetPort: 8888    # â† â˜… Pod å´ã®ãƒãƒ¼ãƒˆã¨â€œã‚ºãƒ¬ã¦ã„ã‚‹â€ãŸã‚é€šä¿¡ã§ããªã„
      protocol: TCP

curl-test.yaml
apiVersion: v1
kind: Pod
metadata:
  name: curl-test
  namespace: mars
spec:
  containers:
    - name: curl
      image: nginx:alpine
      command: ["sh", "-c", "sleep infinity"]
  restartPolicy: Never

# â· ã‚µãƒ¼ãƒ“ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã¿ã‚‹ï¼ˆã¾ã å¤±æ•—ã™ã‚‹ã¯ãšï¼‰
kubectl exec -n mars curl-test -- curl -s --max-time 3 manager-api-svc.mars:4444 || echo "æ¥ç¶šå¤±æ•—"

====================================




====================================
Q19

Question 19:
Solve this question on instance: ssh ckad5601

In Namespace jupiter you'll find an apache Deployment (with one replica) named jupiter-crew-deploy and a ClusterIP Service called jupiter-crew-svc which exposes it.  
Change this Service to a NodePort one to make it available on all nodes on port 30100.

Test the NodePort Service using the internal IP of all available nodes and the port 30100 using curl; you can reach the internal node IPs directly from your main terminal.  
On which nodes is the Service reachable? On which node is the Pod running?

jupiter-namespace.yaml
 apiVersion: v1
kind: Namespace
metadata:
  name: jupiter



jupiter-crew-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jupiter-crew-deploy
  namespace: jupiter
  labels:
    app: jupiter-crew
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jupiter-crew
  template:
    metadata:
      labels:
        app: jupiter-crew
    spec:
      containers:
        - name: apache
          image: httpd:2.4-alpine
          ports:
            - containerPort: 80


jupiter-crew-svc.yaml
 apiVersion: v1
kind: Service
metadata:
  name: jupiter-crew-svc
  namespace: jupiter
spec:
  type: ClusterIP          # â† ã“ã“ã‚’ NodePort ã«ç›´ã—ã€nodePort: 30100 ã‚’è¿½åŠ ã™ã‚‹
  selector:
    app: jupiter-crew
  ports:
    - name: http
      port: 80            # ã‚µãƒ¼ãƒ“ã‚¹ãŒå…¬é–‹ã™ã‚‹ãƒãƒ¼ãƒˆ
      targetPort: 80      # Pod å´ã®ãƒãƒ¼ãƒˆ
      protocol: TCP

====================================




====================================
Q20

Question 20:
Solve this question on instance: ssh ckad7326

In Namespace venus you'll find two Deployments named api and frontend.  
Both Deployments are exposed inside the cluster using Services.  
Create a NetworkPolicy named np1 which restricts outgoing TCP connections from Deployment frontend and only allows those going to Deployment api.  
Make sure the NetworkPolicy still allows outgoing traffic on UDP/TCP port 53 for DNS resolution.

Test using: wget www.google.com and wget api:2222 from a Pod of Deployment frontend.

venus-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: venus


api-deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
  namespace: venus
  labels:
    app: api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: api
  template:
    metadata:
      labels:
        app: api
    spec:
      containers:
        - name: api
          image: busybox:1.31.0            # wget ãŒå…¥ã£ã¦ã„ã¦è»½é‡
          command:
            - sh
            - -c
            - |
              # â€œHTTP/1.1 200 OKâ€ ã‚’è¿”ã™ç°¡æ˜“ã‚µãƒ¼ãƒ
              echo '<h1>api OK</h1>' > /www/index.html
              httpd -f -p 2222 -h /www     # ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ãƒãƒ¼ãƒˆ 2222
          ports:
            - containerPort: 2222
              protocol: TCP



api-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: api
  namespace: venus
spec:
  type: ClusterIP
  selector:
    app: api
  ports:
    - name: http
      port: 2222        # Service ã§è§£æ±ºã•ã‚Œã‚‹ãƒãƒ¼ãƒˆ
      targetPort: 2222  # Pod å´ã®ãƒãƒ¼ãƒˆ
      protocol: TCP


====================================


Question 21:
Solve this question on instance: ssh ckad7326

Team Neptune needs 3 Pods of image httpd:2.4-alpine; create a Deployment named neptune-10ab for this.  
The containers should be named neptune-pod-10ab.  
Each container should have a memory request of 20Mi and a memory limit of 50Mi.

Team Neptune has its own ServiceAccount neptune-sa-v2 under which the Pods should run.  
The Deployment should be in Namespace neptune.


Question 22:
Solve this question on instance: ssh ckad9043

Team Sunny needs to identify some of their Pods in namespace sun.  
They ask you to add a new label protected: true to all Pods with an existing label type: worker or type: runner.  
Also add an annotation protected: "do not delete this pod" to all Pods having the new label protected: true.

namespace.yaml
# neptune / sun ãã‚Œãã‚Œã® Namespace ã‚’ä½œæˆ
apiVersion: v1
kind: Namespace
metadata:
  name: neptune
---
apiVersion: v1
kind: Namespace
metadata:
  name: sun
---
# Neptune ãƒãƒ¼ãƒ ç”¨ ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: neptune-sa-v2
  namespace: neptune

Deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: neptune-10ab
  namespace: neptune
  labels:
    app: neptune-10ab
spec:
  replicas: 1            # â† â˜… ã“ã“ã‚’ 3 ã«ç›´ã™
  selector:
    matchLabels:
      app: neptune-10ab
  template:
    metadata:
      labels:
        app: neptune-10ab
    spec:
      serviceAccountName: neptune-sa-v2
      containers:
        - name: neptune-pod-10ab     # â† â˜… èª²é¡Œã©ãŠã‚Š
          image: httpd:2.4-alpine    # â† â˜… èª²é¡Œã©ãŠã‚Š
          resources:
            requests:
              memory: "20Mi"         # â† â˜… èª²é¡Œã©ãŠã‚Š
            limits:
              memory: "50Mi"         # â† â˜… èª²é¡Œã©ãŠã‚Š
              
pod.yaml
# worker å½¹
apiVersion: v1
kind: Pod
metadata:
  name: worker-a
  namespace: sun
  labels:
    type: worker
spec:
  containers:
    - name: pause
      image: registry.k8s.io/pause:3.9
---
apiVersion: v1
kind: Pod
metadata:
  name: worker-b
  namespace: sun
  labels:
    type: worker
spec:
  containers:
    - name: pause
      image: registry.k8s.io/pause:3.9
---
# runner å½¹
apiVersion: v1
kind: Pod
metadata:
  name: runner-a
  namespace: sun
  labels:
    type: runner
spec:
  containers:
    - name: pause
      image: registry.k8s.io/pause:3.9
---
# ãƒ©ãƒ™ãƒ«ãŒæ¡ä»¶ã«åˆã‚ãªã„ Podï¼ˆå‹•ä½œç¢ºèªç”¨ï¼‰
apiVersion: v1
kind: Pod
metadata:
  name: misc-x
  namespace: sun
  labels:
    type: misc
spec:
  containers:
    - name: pause
      image: registry.k8s.io/pause:3.9

